{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Data handling, processing and visualisations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "# Couple of sklearn operations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Deep learning tools from the keras library\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras import layers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import models\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#for the pickle files(given dataset)\n",
    "#the path written is complete since the script was run on MacOS\n",
    "with open('/Users/saanidhi/Desktop/CV_Problem/train_label.pkl', 'rb') as f:\n",
    "    label = pickle.load(f)\n",
    "with open('/Users/saanidhi/Desktop/CV_Problem/train_image.pkl', 'rb') as f2:\n",
    "    train = pickle.load(f2)\n",
    "with open('/Users/saanidhi/Desktop/CV_Problem/test_image.pkl', 'rb') as f1:\n",
    "    test = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAD8CAYAAACPWyg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHpKkZv8fqB51GcLTPq0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a252acb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Plot graph of labels\n",
    "g = sns.countplot(label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fac = 255  *0.99 + 0.01\n",
    "train_imgs = np.asfarray(train) / fac\n",
    "test_imgs = np.asfarray(test) / fac\n",
    "label=np.asfarray(label)\n",
    "X_trainset=train_imgs.reshape(-1,28,28,1)\n",
    "Test=test_imgs.reshape(-1,28,28,1)\n",
    "#conversion of training and test images also reshaping the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random_seed = 2\n",
    "\n",
    "# Split the train and the validation set for the fitting\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainset, label, test_size = 0.15, random_state=random_seed)\n",
    "y_binary = to_categorical(y_train)\n",
    "y_binaryval = to_categorical(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining the model\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(7, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#declaring an optimizer\n",
    "my_optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for compilation of model\n",
    "model.compile(optimizer=my_optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to change the data\n",
    "datagen = ImageDataGenerator(rotation_range=10,\n",
    "                             zoom_range=0.1,\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             fill_mode='nearest')  \n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#changing the learning rate after certain number of epochs\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 92s - loss: 0.9933 - acc: 0.6466 - val_loss: 0.8200 - val_acc: 0.7592\n",
      "Epoch 2/50\n",
      " - 77s - loss: 0.7555 - acc: 0.7024 - val_loss: 0.5870 - val_acc: 0.7817\n",
      "Epoch 3/50\n",
      " - 83s - loss: 0.6995 - acc: 0.7195 - val_loss: 0.8935 - val_acc: 0.6975\n",
      "Epoch 4/50\n",
      " - 99s - loss: 0.6233 - acc: 0.7462 - val_loss: 0.4759 - val_acc: 0.8208\n",
      "Epoch 5/50\n",
      " - 70s - loss: 0.5952 - acc: 0.7612 - val_loss: 0.6376 - val_acc: 0.7750\n",
      "Epoch 6/50\n",
      " - 83s - loss: 0.5573 - acc: 0.7758 - val_loss: 0.5585 - val_acc: 0.7883\n",
      "Epoch 7/50\n",
      " - 113s - loss: 0.5538 - acc: 0.7737 - val_loss: 0.4927 - val_acc: 0.8142\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      " - 176s - loss: 0.5342 - acc: 0.7861 - val_loss: 0.5317 - val_acc: 0.7975\n",
      "Epoch 9/50\n",
      " - 79s - loss: 0.4945 - acc: 0.8058 - val_loss: 0.3914 - val_acc: 0.8417\n",
      "Epoch 10/50\n",
      " - 70s - loss: 0.4715 - acc: 0.8130 - val_loss: 0.4236 - val_acc: 0.8283\n",
      "Epoch 11/50\n",
      " - 71s - loss: 0.4572 - acc: 0.8244 - val_loss: 0.5452 - val_acc: 0.7750\n",
      "Epoch 12/50\n",
      " - 79s - loss: 0.4558 - acc: 0.8223 - val_loss: 0.3884 - val_acc: 0.8492\n",
      "Epoch 13/50\n",
      " - 87s - loss: 0.4586 - acc: 0.8224 - val_loss: 0.3807 - val_acc: 0.8517\n",
      "Epoch 14/50\n",
      " - 77s - loss: 0.4408 - acc: 0.8246 - val_loss: 0.4029 - val_acc: 0.8408\n",
      "Epoch 15/50\n",
      " - 79s - loss: 0.4423 - acc: 0.8246 - val_loss: 0.4000 - val_acc: 0.8417\n",
      "Epoch 16/50\n",
      " - 70s - loss: 0.4404 - acc: 0.8302 - val_loss: 0.3653 - val_acc: 0.8567\n",
      "Epoch 17/50\n",
      " - 71s - loss: 0.4168 - acc: 0.8357 - val_loss: 0.5005 - val_acc: 0.8208\n",
      "Epoch 18/50\n",
      " - 73s - loss: 0.4439 - acc: 0.8210 - val_loss: 0.3911 - val_acc: 0.8408\n",
      "Epoch 19/50\n",
      " - 74s - loss: 0.4257 - acc: 0.8379 - val_loss: 0.3496 - val_acc: 0.8625\n",
      "Epoch 20/50\n",
      " - 77s - loss: 0.4141 - acc: 0.8409 - val_loss: 0.3338 - val_acc: 0.8675\n",
      "Epoch 21/50\n",
      " - 79s - loss: 0.4064 - acc: 0.8408 - val_loss: 0.4329 - val_acc: 0.8275\n",
      "Epoch 22/50\n",
      " - 75s - loss: 0.4224 - acc: 0.8349 - val_loss: 0.3700 - val_acc: 0.8625\n",
      "Epoch 23/50\n",
      " - 74s - loss: 0.3961 - acc: 0.8472 - val_loss: 0.3476 - val_acc: 0.8583\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      " - 72s - loss: 0.3922 - acc: 0.8466 - val_loss: 0.3477 - val_acc: 0.8667\n",
      "Epoch 25/50\n",
      " - 74s - loss: 0.3797 - acc: 0.8500 - val_loss: 0.3123 - val_acc: 0.8675\n",
      "Epoch 26/50\n",
      " - 70s - loss: 0.3867 - acc: 0.8472 - val_loss: 0.3961 - val_acc: 0.8417\n",
      "Epoch 27/50\n",
      " - 76s - loss: 0.3704 - acc: 0.8594 - val_loss: 0.3183 - val_acc: 0.8742\n",
      "Epoch 28/50\n",
      " - 74s - loss: 0.3654 - acc: 0.8592 - val_loss: 0.3267 - val_acc: 0.8675\n",
      "Epoch 29/50\n",
      " - 74s - loss: 0.3703 - acc: 0.8498 - val_loss: 0.3061 - val_acc: 0.8758\n",
      "Epoch 30/50\n",
      " - 74s - loss: 0.3579 - acc: 0.8634 - val_loss: 0.3274 - val_acc: 0.8633\n",
      "Epoch 31/50\n",
      " - 73s - loss: 0.3984 - acc: 0.8525 - val_loss: 0.3092 - val_acc: 0.8683\n",
      "Epoch 32/50\n",
      " - 73s - loss: 0.3632 - acc: 0.8584 - val_loss: 0.4540 - val_acc: 0.8150\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      " - 75s - loss: 0.3647 - acc: 0.8607 - val_loss: 0.3084 - val_acc: 0.8742\n",
      "Epoch 34/50\n",
      " - 72s - loss: 0.3420 - acc: 0.8679 - val_loss: 0.3114 - val_acc: 0.8750\n",
      "Epoch 35/50\n",
      " - 73s - loss: 0.3424 - acc: 0.8687 - val_loss: 0.3165 - val_acc: 0.8650\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      " - 74s - loss: 0.3495 - acc: 0.8628 - val_loss: 0.3185 - val_acc: 0.8700\n",
      "Epoch 37/50\n",
      " - 77s - loss: 0.3375 - acc: 0.8738 - val_loss: 0.3089 - val_acc: 0.8683\n",
      "Epoch 38/50\n",
      " - 79s - loss: 0.3257 - acc: 0.8715 - val_loss: 0.2984 - val_acc: 0.8775\n",
      "Epoch 39/50\n",
      " - 77s - loss: 0.3363 - acc: 0.8659 - val_loss: 0.3087 - val_acc: 0.8675\n",
      "Epoch 40/50\n",
      " - 78s - loss: 0.3461 - acc: 0.8665 - val_loss: 0.3059 - val_acc: 0.8767\n",
      "Epoch 41/50\n",
      " - 68s - loss: 0.3338 - acc: 0.8688 - val_loss: 0.3043 - val_acc: 0.8717\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      " - 80s - loss: 0.3365 - acc: 0.8688 - val_loss: 0.3271 - val_acc: 0.8658\n",
      "Epoch 43/50\n",
      " - 100s - loss: 0.3396 - acc: 0.8731 - val_loss: 0.3053 - val_acc: 0.8758\n",
      "Epoch 44/50\n",
      " - 83s - loss: 0.3355 - acc: 0.8652 - val_loss: 0.3195 - val_acc: 0.8642\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      " - 85s - loss: 0.3268 - acc: 0.8759 - val_loss: 0.3042 - val_acc: 0.8758\n",
      "Epoch 46/50\n",
      " - 80s - loss: 0.3196 - acc: 0.8790 - val_loss: 0.3102 - val_acc: 0.8675\n",
      "Epoch 47/50\n",
      " - 72s - loss: 0.3351 - acc: 0.8685 - val_loss: 0.3027 - val_acc: 0.8767\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      " - 74s - loss: 0.3187 - acc: 0.8841 - val_loss: 0.3010 - val_acc: 0.8750\n",
      "Epoch 49/50\n",
      " - 74s - loss: 0.3264 - acc: 0.8747 - val_loss: 0.3050 - val_acc: 0.8750\n",
      "Epoch 50/50\n",
      " - 74s - loss: 0.3155 - acc: 0.8763 - val_loss: 0.3014 - val_acc: 0.8758\n"
     ]
    }
   ],
   "source": [
    "#training the model with epochs as 50\n",
    "history = model.fit_generator(datagen.flow(X_train, y_binary, batch_size=86),\n",
    "                              epochs=50, validation_data = (X_val, y_binaryval),\n",
    "                              verbose = 2, steps_per_epoch=X_train.shape[0] // 86,\n",
    "                              callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the results in result\n",
    "result=(model.predict(Test,batch_size=None, verbose=0, steps=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reversing the to_categorical \n",
    "y_classes = [np.argmax(y, axis=None, out=None) for y in result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying the value of predicted class\n",
    "y_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the values into a csv file \n",
    "import csv\n",
    " \n",
    "with open('/Users/saanidhi/Desktop/CV_Problem/saanidhiarora.csv', 'w') as csvfile: #since the script was run at macos so complete path needs to be defined\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow(['Testimageindex', 'Predictedclass'])\n",
    "    i=0\n",
    "    for x in y_classes :\n",
    "       filewriter.writerow([i, x])\n",
    "       i=i+1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
